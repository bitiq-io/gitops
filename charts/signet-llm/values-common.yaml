name: signet-llm

serviceAccount:
  create: false
  name: ""

predictor:
  minReplicas: 1
  maxReplicas: 1
  runtimeClassName: ""
  deploymentStrategy: {}
  nodeSelector:
    nvidia.com/gpu.present: "true"
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  affinity: {}

model:
  runtime: vllm-runtime
  modelFormat: vllm
  storageUri: ""
  servedModelName: signet-llm
  args:
    - --served-model-name=signet-llm
    - --max-model-len=8192
  env: []
  auth:
    enabled: false
    secretName: huggingface-token
    tokenKey: token
    envName: HUGGING_FACE_HUB_TOKEN

runtime:
  create: true
  name: vllm-runtime
  image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768
  imagePullPolicy: IfNotPresent
  protocolVersions:
    - v2
  supportedModelFormats:
    - name: vllm
      version: "1"
      autoSelect: true
  container:
    name: kserve-container
    command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
    args:
      - --model
      - /mnt/models
      - --port
      - "8080"
      - --host
      - "0.0.0.0"
    env: []

resources:
  requests:
    cpu: "2"
    memory: 16Gi
    nvidia.com/gpu: "1"
  limits:
    cpu: "4"
    memory: 32Gi
    nvidia.com/gpu: "1"

route:
  enabled: false
  host: ""
  serviceName: ""
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
