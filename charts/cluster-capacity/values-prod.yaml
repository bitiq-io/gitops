enabled: true

# Cluster infraID (infrastructureName)
infraID: prod-nkqd9

# AWS accelerator quotas are vCPU-based. With `Running On-Demand G and VT instances = 8`,
# the highest GPU count you can run without a quota increase is:
# - 2x g6e.xlarge (4 vCPU each, 1x L40S 48GB each)
#
# To enable a pool:
# 1) Copy an existing worker MachineSet providerSpec from the cluster
# 2) Paste it under machineSets[].providerSpec
# 3) Update at least: instanceType and placement.availabilityZone
# 4) Ensure spotMarketOptions is absent for on-demand
# 4) Flip machineSets[].enabled=true (and chart enabled=true when ready)
machineSets:
  # NVIDIA GPU pool (general-purpose inference)
  - enabled: true
    nameSuffix: gpu-g6e-xlarge-us-west-2a
    replicas: 2
    machineRole: gpu
    machineType: gpu
    nodeLabels:
      node-role.kubernetes.io/worker: ""
      node-role.kubernetes.io/gpu: ""
    taints:
      - key: nvidia.com/gpu
        value: "true"
        effect: NoSchedule
    autoscaler:
      enabled: false
      minReplicas: 0
      maxReplicas: 2
    providerSpec:
      ami:
        id: ami-0d769ba340e913a8c
      apiVersion: machine.openshift.io/v1beta1
      blockDevices:
        - ebs:
            encrypted: true
            iops: 0
            kmsKey:
              arn: ""
            volumeSize: 120
            volumeType: gp3
      capacityReservationId: ""
      credentialsSecret:
        name: aws-cloud-credentials
      deviceIndex: 0
      iamInstanceProfile:
        id: prod-nkqd9-worker-profile
      instanceType: g6e.xlarge
      kind: AWSMachineProviderConfig
      metadata:
        creationTimestamp: null
      metadataServiceOptions: {}
      placement:
        availabilityZone: us-west-2a
        region: us-west-2
      securityGroups:
        - filters:
            - name: tag:Name
              values:
                - prod-nkqd9-node
        - filters:
            - name: tag:Name
              values:
                - prod-nkqd9-lb
      subnet:
        filters:
          - name: tag:Name
            values:
              - prod-nkqd9-subnet-private-us-west-2a
      tags:
        - name: kubernetes.io/cluster/prod-nkqd9
          value: owned
      userDataSecret:
        name: worker-user-data

  # AWS Inferentia2 pool (gated; verify Neuron support on OpenShift/RHCOS first)
  - enabled: false
    nameSuffix: inf-inf2-xlarge-us-west-2a
    replicas: 0
    machineRole: inferentia
    machineType: inferentia
    nodeLabels:
      node-role.kubernetes.io/worker: ""
      node-role.kubernetes.io/inferentia: ""
    taints:
      - key: aws.amazon.com/neuron
        value: "true"
        effect: NoSchedule
    autoscaler:
      enabled: false
      minReplicas: 0
      maxReplicas: 2
    providerSpec:
      ami:
        id: ami-0d769ba340e913a8c
      apiVersion: machine.openshift.io/v1beta1
      blockDevices:
        - ebs:
            encrypted: true
            iops: 0
            kmsKey:
              arn: ""
            volumeSize: 120
            volumeType: gp3
      capacityReservationId: ""
      credentialsSecret:
        name: aws-cloud-credentials
      deviceIndex: 0
      iamInstanceProfile:
        id: prod-nkqd9-worker-profile
      instanceType: inf2.xlarge
      kind: AWSMachineProviderConfig
      metadata:
        creationTimestamp: null
      metadataServiceOptions: {}
      placement:
        availabilityZone: us-west-2a
        region: us-west-2
      securityGroups:
        - filters:
            - name: tag:Name
              values:
                - prod-nkqd9-node
        - filters:
            - name: tag:Name
              values:
                - prod-nkqd9-lb
      subnet:
        filters:
          - name: tag:Name
            values:
              - prod-nkqd9-subnet-private-us-west-2a
      tags:
        - name: kubernetes.io/cluster/prod-nkqd9
          value: owned
      userDataSecret:
        name: worker-user-data

# NVIDIA GPU Operator (enable after the first GPU node is Ready)
nvidia:
  enabled: true
  dashboards:
    enabled: true
  subscription:
    channel: v25.10
